import json
from typing import List, Dict, Optional
from pathlib import Path


class PromptManager:
    """
    GÃ¨re le chargement, le stockage et le formatage des prompts pour tous les agents.
    Injecte dynamiquement le contexte (code, rÃ©sultats, tests) dans les templates.
    """

    def __init__(self, templates_dir: str = "prompts"):
        """
        Initialisation du gestionnaire de prompts.

        Args:
            templates_dir: RÃ©pertoire contenant les fichiers de templates
        """
        self.templates_dir = Path(templates_dir)
        self.templates_cache: Dict[str, str] = {}
        self._load_templates()

    def _load_templates(self):
        """Charge tous les templates depuis le disque"""
        files_map = {
            "auditor": "auditor_prompt.txt",
            "fixer": "fixer_prompt.txt",
            "judge": "judge_prompt.txt"
        }
        for agent, filename in files_map.items():
            file_path = self.templates_dir / filename
            if file_path.exists():
                with open(file_path, encoding="utf-8") as f:
                    self.templates_cache[agent] = f.read()
                print(f"âœ… Template chargÃ© pour {agent}")
            else:
                print(f"âš ï¸ Template manquant : {filename}")
                self.templates_cache[agent] = ""

    # =================== AUDITOR ===================

    def build_auditor_prompt(self, file_name: str, content: str, lint_data: Optional[Dict] = None) -> str:
        """
        PrÃ©pare le prompt pour l'agent Auditor.

        Args:
            file_name: Chemin du fichier
            content: Contenu du code
            lint_data: RÃ©sultats de lint optionnels (pylint)

        Returns:
            Prompt complet prÃªt Ã  envoyer au LLM
        """
        template = self.templates_cache.get("auditor", "")
        context = f"FICHIER: {file_name}\n\nCODE:\n```python\n{content}\n```\n"

        if lint_data and lint_data.get("success"):
            context += f"\nLINT:\n- Score: {lint_data.get('score', 'N/A')}/10\n"
            issues = lint_data.get("categorized", {})
            context += f"- Erreurs: {len(issues.get('error', []))}\n- Avertissements: {len(issues.get('warning', []))}\n"
            context += "- Top problÃ¨mes:\n"
            for i, issue in enumerate(lint_data.get("issues", [])[:5], 1):
                context += f"{i}. Ligne {issue.get('line', '?')}: {issue.get('message', 'Inconnu')}\n"

        return f"{template}\n\n{context}\nVeuillez fournir votre analyse au format JSON."

    # =================== FIXER ===================

    def build_fixer_prompt(self, file_name: str, content: str, plan: List[Dict], prev_errors: Optional[List[str]] = None) -> str:
        """
        PrÃ©pare le prompt pour l'agent Fixer avec le plan de refactoring.

        Args:
            file_name: Fichier Ã  corriger
            content: Contenu actuel du fichier
            plan: Plan de refactoring issu de l'Auditor
            prev_errors: Erreurs prÃ©cÃ©dentes pour self-healing

        Returns:
            Prompt complet pour corriger le code
        """
        template = self.templates_cache.get("fixer", "")
        context = f"FICHIER Ã€ CORRIGER: {file_name}\n\nCODE ACTUEL:\n```python\n{content}\n```\n\nPLAN DE REFACTORING:\n"
        for idx, step in enumerate(plan, 1):
            context += f"{idx}. {step.get('step', 'Corriger problÃ¨me')}\n"
            if step.get('rationale'):
                context += f"   Raison: {step['rationale']}\n"

        if prev_errors:
            context += "\nERREURS PRÃ‰CÃ‰DENTES:\n"
            for e in prev_errors:
                context += f"- {e}\n"
            context += "\nVeuillez corriger ces erreurs Ã©galement.\n"

        context += "\nIMPORTANT:\n- Retourner uniquement le code corrigÃ©, sans explications.\n- Respecter PEP8 et ajouter les docstrings manquantes.\n"
        context += "Fournir le code complet corrigÃ© :"
        return f"{template}\n\n{context}"

    def build_fixer_prompt_from_tests(self, file_name: str, content: str, test_log: str, test_stats: Dict) -> str:
        """
        Prompt pour Fixer basÃ© sur les rÃ©sultats de tests Ã©chouÃ©s.

        Args:
            file_name: Fichier ayant Ã©chouÃ© aux tests
            content: Contenu actuel
            test_log: Log de pytest
            test_stats: Statistiques des tests

        Returns:
            Prompt complet pour correction
        """
        template = self.templates_cache.get("fixer", "")
        context = f"FICHIER: {file_name}\n\nCODE ACTUEL:\n```python\n{content}\n```\n"
        context += f"\nTESTS:\n- PassÃ©s: {test_stats.get('passed', 0)}\n- Ã‰chouÃ©s: {test_stats.get('failed', 0)}\n- Erreurs: {test_stats.get('errors', 0)}\n\n"
        context += f"LOG TEST:\n```\n{test_log}\n```\n"
        context += "Analysez les erreurs et corrigez le code pour que tous les tests passent.\nFournir le code complet corrigÃ© :"
        return f"{template}\n\n{context}"

    # =================== JUDGE ===================

    def build_judge_prompt(self, test_log: str, stats: Dict, prev_score: Optional[float] = None, current_score: Optional[float] = None) -> str:
        """
        PrÃ©pare le prompt pour l'agent Judge basÃ© sur les rÃ©sultats de tests.

        Args:
            test_log: Log complet des tests
            stats: Statistiques des tests
            prev_score: Ancien score Pylint
            current_score: Score Pylint actuel

        Returns:
            Prompt complet pour juger la mission
        """
        template = self.templates_cache.get("judge", "")
        context = f"RÃ‰SULTATS DES TESTS:\n- PassÃ©s: {stats.get('passed', 0)}\n- Ã‰chouÃ©s: {stats.get('failed', 0)}\n- Erreurs: {stats.get('errors', 0)}\n- Total: {stats.get('total', 0)}\n"
        if prev_score is not None and current_score is not None:
            improvement = current_score - prev_score
            context += f"\nSCORE PYLINT:\n- Ancien: {prev_score}/10\n- Actuel: {current_score}/10\n- AmÃ©lioration: {improvement:+.2f}\n"
        elif current_score is not None:
            context += f"\nSCORE PYLINT:\n- Actuel: {current_score}/10\n- Ancien: N/A\n- AmÃ©lioration: N/A\n"
        else:
            context += "\nSCORE PYLINT: N/A\n"

        context += f"\nLOG COMPLET DES TESTS:\n```\n{test_log}\n```\n"
        context += "Analysez et indiquez si le code est prÃªt. Fournir votre verdict en JSON :"
        return f"{template}\n\n{context}"

    # =================== UTILITAIRES ===================

    def truncate_prompt(self, prompt: str, max_lines: int = 100) -> str:
        """
        RÃ©duit un prompt trop long pour Ã©conomiser des tokens LLM.

        Args:
            prompt: Texte complet du prompt
            max_lines: Nombre maximal de lignes conservÃ©es

        Returns:
            Prompt tronquÃ©
        """
        lines = prompt.splitlines()
        if len(lines) <= max_lines:
            return prompt
        half = max_lines // 2
        return "\n".join(lines[:half] + ["\n... [TRONQUÃ‰] ...\n"] + lines[-half:])

    def parse_json_response(self, response: str) -> Optional[Dict]:
        """
        Extrait du JSON d'une rÃ©ponse LLM, supprime les blocs markdown.

        Args:
            response: Texte brut LLM

        Returns:
            Dict JSON ou None
        """
        cleaned = response.strip().lstrip("```json").lstrip("```").rstrip("```").strip()
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            print("âŒ Erreur JSON dÃ©tectÃ©e dans la rÃ©ponse LLM")
            return None

    def get_template(self, agent: str) -> str:
        """Retourne le template brut pour un agent"""
        return self.templates_cache.get(agent, "")

    def reload_templates(self):
        """Recharge tous les templates depuis le disque"""
        self.templates_cache.clear()
        self._load_templates()
        print("ğŸ”„ Templates rechargÃ©s avec succÃ¨s")
