import os
import sys
import argparse
import time  # Import indispensable pour les pauses
from pathlib import Path
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI

# -----------------------------
# PATH CONFIG
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(BASE_DIR, "src"))

from src.utils.logger import log_experiment, ActionType
from src.utils.toolsmith_utils import run_pylint, run_pytest, lire_fichier, ecrire_fichier
from src.prompts.PromptManager import PromptManager

# -----------------------------
# ENV
# -----------------------------
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    print("âŒ ClÃ© API manquante (.env)")
    sys.exit(1)

# -----------------------------
# LLM
# -----------------------------
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash",
    google_api_key=api_key,
    temperature=0,
    verbose=True
)

# =====================================================
# ORCHESTRATEUR (Audit â†’ Fix â†’ Test â†’ Loop)
# =====================================================
def orchestrator(file_path, max_iterations):
    pm = PromptManager()
    abs_path = os.path.abspath(file_path)
    current_score = 0  # Suivi du score de qualitÃ©

    print(f"\nğŸš€ [MISSION] {file_path}")

    for iteration in range(1, max_iterations + 1):
        print(f"\nğŸ” ITERATION {iteration}/{max_iterations}")

        # =====================================
        # 1ï¸âƒ£ AUDIT
        # =====================================
        try:
            code_original = lire_fichier(abs_path)
            lint = run_pylint(abs_path)
            
            # RÃ©cupÃ©ration du score pour l'IA
            current_score = lint.get("score", 0)
            print(f"ğŸ“Š QualitÃ© actuelle : {current_score}/10")

            prompt = pm.build_auditor_prompt(file_path, code_original, lint)
            response = llm.invoke(prompt)

            log_experiment(
                "Auditor",
                "models/gemini-2.5-flash",
                ActionType.ANALYSIS,
                {
                    "file": file_path,
                    "input_prompt": prompt,
                    "output_response": response.content,
                    "score": current_score
                },
                "SUCCESS"
            )

            analyse = pm.parse_json_response(response.content)
            plan = analyse.get("refactoring_plan", [])
            print(f"âœ… Audit OK ({len(plan)} problÃ¨mes dÃ©tectÃ©s)")

            # Petite pause pour Ã©viter l'erreur 429 entre deux appels
            time.sleep(5)

        except Exception as e:
            print(f"âŒ Audit failed: {e}")
            return

        # =====================================
        # 2ï¸âƒ£ FIXER
        # =====================================
        try:
            prompt_fix = pm.build_fixer_prompt(file_path, code_original, plan)
            response_fix = llm.invoke(prompt_fix)

            log_experiment(
                "Fixer",
                "models/gemini-2.5-flash",
                ActionType.FIX,
                {
                    "file": file_path,
                    "input_prompt": prompt_fix,
                    "output_response": response_fix.content
                },
                "SUCCESS"
            )

            data = pm.parse_json_response(response_fix.content)

            if data and "code_corrige" in data:
                ecrire_fichier(abs_path, data["code_corrige"])
                print("ğŸ“ Code corrigÃ© Ã©crit")

        except Exception as e:
            print(f"âŒ Fix failed: {e}")
            return

        # =====================================
        # 3ï¸âƒ£ JUDGE (pytest)
        # =====================================
        print("ğŸ§ª Running tests...")
        # Pause avant les tests pour laisser le systÃ¨me de fichiers respirer
        time.sleep(2)
        result_pytest = run_pytest(abs_path)
        
        # Logique flexible basÃ©e sur le "status" renvoyÃ© par toolsmith_utils
        success = False
        logs = "Aucun log"

        if isinstance(result_pytest, dict):
            # On utilise le 'status' SUCCESS/FAILURE qu'on a dÃ©fini ensemble
            success = result_pytest.get("status") == "SUCCESS"
            logs = result_pytest.get("stdout", "Aucun log")
        else:
            success = result_pytest[0]
            logs = result_pytest[1]

        log_experiment(
            agent_name="Judge",
            model_used="pytest",
            action=ActionType.DEBUG,
            details={
                "file": file_path,
                "input_prompt": "ExÃ©cution des tests unitaires",
                "output_response": str(logs)
            },
            status="SUCCESS" if success else "FAILURE"
        )

        if success:
            # Si le score est parfait ou les tests passent, on s'arrÃªte
            if current_score >= 9.5:
                print(f"ğŸ‰ MISSION ACCOMPLIE (Score: {current_score}/10) â†’ fichier validÃ©")
                return
            else:
                print(f"âœ… Tests OK, mais score Pylint ({current_score}) amÃ©liorable. ItÃ©ration suivante...")
        else:
            print(f"âŒ Tests FAIL ou Code incomplet â†’ nouvelle tentative")

        # Pause de fin d'itÃ©ration pour reset le quota Gemini
        print("â³ Pause de 10s pour le quota API...")
        time.sleep(10)

    print("âš ï¸ Max iterations atteintes â†’ fin de mission")

# =====================================================
# MAIN CLI (Reste inchangÃ©)
# =====================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--target_dir", required=True)
    parser.add_argument("--max_iterations", type=int, default=5)

    args = parser.parse_args()
    print("ğŸ¤– Refactoring Swarm dÃ©marrÃ©")
    target = Path(args.target_dir)

    if target.is_file():
        orchestrator(str(target), args.max_iterations)
    elif target.is_dir():
        for f in target.glob("*.py"):
            orchestrator(str(f), args.max_iterations)
    else:
        print("âŒ Chemin invalide")

if __name__ == "__main__":
    main()